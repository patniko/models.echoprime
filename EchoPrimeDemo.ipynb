{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd78301",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"model_data/example_study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adef3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Third-party library imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pydicom\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "# Local module imports\n",
    "import utils\n",
    "import video_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c561a",
   "metadata": {},
   "source": [
    "## Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cd8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc584b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m echo_encoder.parameters():\n\u001b[32m      8\u001b[39m     param.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m vc_checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_data/weights/view_classifier.ckpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m vc_state_dict={key[\u001b[32m6\u001b[39m:]:value \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m vc_checkpoint[\u001b[33m'\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m'\u001b[39m].items()}\n\u001b[32m     12\u001b[39m view_classifier = torchvision.models.convnext_base()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:1516\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1524\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:2114\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2113\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2114\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2115\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2117\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/_weights_only_unpickler.py:532\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    525\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ):\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    530\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    534\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:2078\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2076\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2077\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2078\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:2044\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2040\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   2041\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._guards.detect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2044\u001b[39m     wrap_storage = \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2046\u001b[39m     storage._fake_device = location\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:698\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    680\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    696\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    700\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:636\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    634\u001b[39m     backend_name = torch._C._get_privateuse1_backend_name()\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     device = \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.to(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/echoprime-6Nm4Obin-py3.13/lib/python3.13/site-packages/torch/serialization.py:605\u001b[39m, in \u001b[36m_validate_device\u001b[39m\u001b[34m(location, backend_name)\u001b[39m\n\u001b[32m    603\u001b[39m     device_index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mis_available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    606\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    607\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.is_available() is False. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    611\u001b[39m     )\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[33m\"\u001b[39m\u001b[33mdevice_count\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    613\u001b[39m     device_count = device_module.device_count()\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_data/weights/echo_prime_encoder.pt\",map_location=device)\n",
    "echo_encoder = torchvision.models.video.mvit_v2_s()\n",
    "echo_encoder.head[-1] = torch.nn.Linear(echo_encoder.head[-1].in_features, 512)\n",
    "echo_encoder.load_state_dict(checkpoint)\n",
    "echo_encoder.eval()\n",
    "echo_encoder.to(device)\n",
    "for param in echo_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "vc_checkpoint = torch.load(\"model_data/weights/view_classifier.ckpt\",map_location=device)\n",
    "vc_state_dict={key[6:]:value for key,value in vc_checkpoint['state_dict'].items()}\n",
    "view_classifier = torchvision.models.convnext_base()\n",
    "view_classifier.classifier[-1] = torch.nn.Linear(\n",
    "    view_classifier.classifier[-1].in_features, 11\n",
    ")\n",
    "view_classifier.load_state_dict(vc_state_dict)\n",
    "view_classifier.to(device)\n",
    "view_classifier.eval()\n",
    "for param in view_classifier.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5a52c",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_take=32\n",
    "frame_stride=2\n",
    "video_size=224\n",
    "mean = torch.tensor([29.110628, 28.076836, 29.096405]).reshape(3, 1, 1, 1)\n",
    "std = torch.tensor([47.989223, 46.456997, 47.20083]).reshape(3, 1, 1, 1)\n",
    "\n",
    "def process_dicoms(INPUT):\n",
    "    \"\"\"\n",
    "    Reads DICOM video data from the specified folder and returns a tensor \n",
    "    formatted for input into the EchoPrime model.\n",
    "\n",
    "    Args:\n",
    "        INPUT (str): Path to the folder containing DICOM files.\n",
    "\n",
    "    Returns:\n",
    "        stack_of_videos (torch.Tensor): A float tensor of shape  (N, 3, 16, 224, 224)\n",
    "                                        representing the video data where N is the number of videos,\n",
    "                                        ready to be fed into EchoPrime.\n",
    "    \"\"\"\n",
    "\n",
    "    dicom_paths = glob.glob(f'{INPUT}/**/*.dcm',recursive=True)\n",
    "    stack_of_videos=[]\n",
    "    for idx, dicom_path in tqdm(enumerate(dicom_paths),total=len(dicom_paths)):\n",
    "        try:\n",
    "            # simple dicom_processing\n",
    "            dcm=pydicom.dcmread(dicom_path)\n",
    "            pixels = dcm.pixel_array\n",
    "            \n",
    "            # exclude images like (600,800) or (600,800,3)\n",
    "            if pixels.ndim < 3 or pixels.shape[2]==3:\n",
    "                continue \n",
    "                \n",
    "            # if single channel repeat to 3 channels    \n",
    "            if pixels.ndim==3:\n",
    "                \n",
    "                pixels = np.repeat(pixels[..., None], 3, axis=3)\n",
    "            \n",
    "            # mask everything outside ultrasound region\n",
    "            pixels=video_utils.mask_outside_ultrasound(dcm.pixel_array)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #model specific preprocessing\n",
    "            x = np.zeros((len(pixels),224,224,3))\n",
    "            for i in range(len(x)):\n",
    "                x[i] = video_utils.crop_and_scale(pixels[i])\n",
    "            \n",
    "            x = torch.as_tensor(x, dtype=torch.float).permute([3,0,1,2])\n",
    "            # normalize\n",
    "            x.sub_(mean).div_(std)\n",
    "        \n",
    "            ## if not enough frames add padding\n",
    "            if x.shape[1] < frames_to_take:\n",
    "                padding = torch.zeros(\n",
    "                (\n",
    "                    3,\n",
    "                    frames_to_take - x.shape[1],\n",
    "                    video_size,\n",
    "                    video_size,\n",
    "                ),\n",
    "                dtype=torch.float,\n",
    "                )\n",
    "                x = torch.cat((x, padding), dim=1)\n",
    "                \n",
    "            start=0\n",
    "            stack_of_videos.append(x[:, start : ( start + frames_to_take) : frame_stride, : , : ])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"corrupt file\")\n",
    "            print(str(e))\n",
    "\n",
    "    stack_of_videos=torch.stack(stack_of_videos)\n",
    "    \n",
    "    return stack_of_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_videos(stack_of_videos):\n",
    "    \"\"\"\n",
    "    Given a set of videos that belong to one echocardiogram study,\n",
    "    embed them in the latent space using EchoPrime encoder\n",
    "    \n",
    "    Args:\n",
    "        stack_of_videos (torch.Tensor): A float tensor of shape (N, 3, 16, 224, 224)\n",
    "                                        with preprocessed echo video data\n",
    "        \n",
    "    Returns:\n",
    "        stack_of_features (torch.Tensor) A float tensor of shape (N, 512)\n",
    "                                         with latent embeddings corresponding to echo videos\n",
    "    \"\"\"\n",
    "    bin_size=50\n",
    "    n_bins=math.ceil(stack_of_videos.shape[0]/bin_size)\n",
    "    stack_of_features_list=[]\n",
    "    with torch.no_grad():\n",
    "        for bin_idx in range(n_bins):\n",
    "            start_idx = bin_idx * bin_size\n",
    "            end_idx = min( (bin_idx + 1) * bin_size, stack_of_videos.shape[0])\n",
    "            bin_videos = stack_of_videos[start_idx:end_idx].to(device)\n",
    "            bin_features = echo_encoder(bin_videos)\n",
    "            stack_of_features_list.append(bin_features)\n",
    "        stack_of_features=torch.cat(stack_of_features_list,dim=0)\n",
    "    return stack_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_views(stack_of_videos, visualize=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        stack_of_videos (torch.Tensor): A float tensor with preprocessed echo video data\n",
    "        \n",
    "    Returns:\n",
    "        stack_of_view_encodings (torch.Tensor) A float tensor of one hot embeddings with shape (N, 11)\n",
    "                                               representing echocardiogram views\n",
    "    \"\"\"\n",
    "    ## get views   \n",
    "    stack_of_first_frames = stack_of_videos[:,:,0,:,:].to(device)\n",
    "    with torch.no_grad():\n",
    "        out_logits=view_classifier(stack_of_first_frames)\n",
    "    out_views=torch.argmax(out_logits,dim=1)\n",
    "    view_list = [utils.COARSE_VIEWS[v] for v in out_views]\n",
    "    stack_of_view_encodings = torch.stack([torch.nn.functional.one_hot(out_views,11)]).squeeze().to(device)\n",
    "\n",
    "    # visualize images and the assigned views\n",
    "    if visualize:\n",
    "        print(\"Preprocessed and normalized video inputs\")\n",
    "        rows, cols = (len(view_list) // 12 + (len(view_list) % 9 > 0)), 12\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "        axes = axes.flatten()\n",
    "        for i in range(len(view_list)):\n",
    "            display_image = (stack_of_first_frames[i].cpu().permute([1,2,0]) * 255).numpy()\n",
    "            display_image = np.clip(display_image, 0, 255).astype('uint8')\n",
    "            display_image = np.ascontiguousarray(display_image)\n",
    "            display_image = cv2.cvtColor(display_image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.putText(display_image, view_list[i].replace(\"_\",\" \"), (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 220, 255), 2)\n",
    "            axes[i].imshow(display_image)\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    return stack_of_view_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_study(INPUT,visualize=False):\n",
    "    \"\"\"\n",
    "    Produces an EchoPrime embedding of the echocardiography study\n",
    "\n",
    "    Args:\n",
    "        INPUT (str): Path to the folder containing DICOM files.\n",
    "        \n",
    "    Returns:\n",
    "        encoded_study (torch.Tensor): A float tensor of shape (N, 523)\n",
    "    \"\"\"\n",
    "    stack_of_videos=process_dicoms(INPUT)\n",
    "    stack_of_features=embed_videos(stack_of_videos)\n",
    "    stack_of_view_encodings=get_views(stack_of_videos,visualize)\n",
    "    encoded_study = torch.cat( (stack_of_features ,stack_of_view_encodings),dim=1)\n",
    "    \n",
    "    return encoded_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723395ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_study=encode_study(INPUT, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6613c9e",
   "metadata": {},
   "source": [
    "# Obtain Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MIL weights per section\n",
    "MIL_weights = pd.read_csv(\"MIL_weights.csv\")\n",
    "non_empty_sections=MIL_weights['Section']\n",
    "section_weights=MIL_weights.iloc[:,1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load candidate reports\n",
    "candidate_studies=list(pd.read_csv(\"model_data/candidates_data/candidate_studies.csv\")['Study'])\n",
    "candidate_embeddings_p1=torch.load(\"model_data/candidates_data/candidate_embeddings_p1.pt\")\n",
    "candidate_embeddings_p2=torch.load(\"model_data/candidates_data/candidate_embeddings_p2.pt\")\n",
    "candidate_embeddings=torch.cat((candidate_embeddings_p1,candidate_embeddings_p2),dim=0)\n",
    "# Optionally normalize each candidate embedding vector\n",
    "# candidate_embeddings = torch.nn.functional.normalize(candidate_embeddings,dim=1)\n",
    "candidate_reports=pd.read_pickle(\"model_data/candidates_data/candidate_reports.pkl\")\n",
    "candidate_reports = [utils.phrase_decode(vec_phr) for vec_phr in tqdm(candidate_reports)]\n",
    "candidate_labels = pd.read_pickle(\"model_data/candidates_data/candidate_labels.pkl\")\n",
    "section_to_phenotypes = pd.read_pickle(\"section_to_phenotypes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13585355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(study_embedding: torch.Tensor) -> str:\n",
    "    \"\"\"\n",
    "    Given the EchoPrime study embedding generate a report\n",
    "    for each section focus on the views weighted\n",
    "    Args:\n",
    "        study_embedding - torch tensor of shape num_videos x 572\n",
    "        original_report - text for original study\n",
    "    \"\"\"\n",
    "    study_embedding=study_embedding.cpu()\n",
    "    generated_report=\"\"\n",
    "    for s_dx, sec in enumerate(non_empty_sections):\n",
    "        # need to multiply it based on what section does the view belong to.\n",
    "        cur_weights=[section_weights[s_dx][torch.where(ten==1)[0]] for ten in study_embedding[:,512:]]\n",
    "        no_view_study_embedding = study_embedding[:,:512] * torch.tensor(cur_weights,dtype=torch.float).unsqueeze(1)\n",
    "        # weights by views.\n",
    "        no_view_study_embedding=torch.mean(no_view_study_embedding,dim=0)\n",
    "        no_view_study_embedding=torch.nn.functional.normalize(no_view_study_embedding,dim=0)\n",
    "        similarities=no_view_study_embedding @ candidate_embeddings.T\n",
    "        \n",
    "        extracted_section=\"Section not found.\"\n",
    "        while extracted_section==\"Section not found.\":\n",
    "            max_id = torch.argmax(similarities)\n",
    "            predicted_section = candidate_reports[max_id]\n",
    "            extracted_section = utils.extract_section(predicted_section,sec)\n",
    "            if extracted_section != \"Section not found.\":\n",
    "                generated_report+= extracted_section\n",
    "            similarities[max_id]=float('-inf')\n",
    "            \n",
    "    return generated_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_metrics(study_embedding: torch.Tensor,\n",
    "                   k=50) -> dict:\n",
    "    \"\"\"\n",
    "    study_embedding is a set of embeddings of all videos from the study e.g (52,512)\n",
    "    Takes a study embedding as input and\n",
    "    outputs a dictionary for a set of 26 features\n",
    "    \"\"\"\n",
    "    #per_section_study_embedding has shape (15,512)\n",
    "    per_section_study_embedding=torch.zeros(len(non_empty_sections),512)\n",
    "    study_embedding=study_embedding.cpu()\n",
    "    # make per section study embedding\n",
    "    for s_dx, sec in enumerate(non_empty_sections):\n",
    "        # get section weights\n",
    "        this_section_weights=[section_weights[s_dx][torch.where(view_encoding==1)[0]]\n",
    "                      for view_encoding in study_embedding[:,512:]]\n",
    "        this_section_study_embedding = (study_embedding[:,:512] * \\\n",
    "                                        torch.tensor(this_section_weights,\n",
    "                                                     dtype=torch.float).unsqueeze(1))\n",
    "        \n",
    "        #weighted average\n",
    "        this_section_study_embedding=torch.sum(this_section_study_embedding,dim=0)\n",
    "        per_section_study_embedding[s_dx]=this_section_study_embedding\n",
    "        \n",
    "    per_section_study_embedding=torch.nn.functional.normalize(per_section_study_embedding)\n",
    "    #similarities has shape (15,230676)\n",
    "    similarities=per_section_study_embedding @ candidate_embeddings.T\n",
    "\n",
    "    # for each row find indices of 50 highest values\n",
    "    #top_candidate_ids has shape (15,50)\n",
    "    top_candidate_ids=torch.topk(similarities, k=k, dim=1).indices\n",
    "    #now predict for each phenotype:\n",
    "    preds={}\n",
    "    for s_dx, section in enumerate(section_to_phenotypes.keys()):\n",
    "        for pheno in section_to_phenotypes[section]:\n",
    "            preds[pheno] = np.nanmean([candidate_labels[pheno][candidate_studies[c_ids]]\n",
    "                                   for c_ids in top_candidate_ids[s_dx]\n",
    "                                      if candidate_studies[c_ids] in candidate_labels[pheno]])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_report(encoded_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature logits\")\n",
    "predict_metrics(encoded_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83079d",
   "metadata": {},
   "source": [
    "It's good to pick ROC thresholds based on a val set from your institution. \n",
    "\n",
    "We provide ROC thresholds that maximize true positive rate and minize false positive rate on val studies from Cedars Sinai Medical Center in roc_thresholds.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1413c9-01b0-4fe0-8a60-21dafa959a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
